@article{hardy2003online,
  title={Online ratings: Fact and fiction},
  author={Hardy, Nedra},
  journal={New directions for teaching and learning},
  volume={2003},
  number={96},
  pages={31--38},
  year={2003}
}


@article{ALHIJA200937,
title = {Student evaluation of instruction: What can be learned from students’ written comments?},
journal = {Studies in Educational Evaluation},
volume = {35},
number = {1},
pages = {37-44},
year = {2009},
issn = {0191-491X},
doi = {https://doi.org/10.1016/j.stueduc.2009.01.002},
url = {https://www.sciencedirect.com/science/article/pii/S0191491X09000066},
author = {Fadia Nasser-Abu Alhija and Barbara Fresko},
abstract = {Extensive research has been done on student ratings of instruction on closed-ended questionnaires, but little research has examined students’ written responses to open-ended questions. This study investigated the written comments of students in 198 classes, focusing on their frequency, content, direction, and consistency with quantitative ratings on closed-ended items. Results indicated that about 45% of the students wrote comments. Comments were more often positive than negative and tended to be general rather than specific. Written comments addressed dimensions similar to those identified in the closed-ended items, but they also related to unique aspects of the courses as well.}
}



@article{stoesz2022bias,
  title={Bias in student ratings of instruction: a systematic review of research from 2012 to 2021},
  author={Stoesz, Brenda M and De Jaeger, Amy E and Quesnel, Matthew and Bhojwani, Dimple and Los, Ryan},
  journal={Canadian Journal of Educational Administration and Policy},
  number={201},
  pages={39--62},
  year={2022},
  publisher={{\'E}rudit},
  url = {https://files.eric.ed.gov/fulltext/EJ1379777.pdf},
  note = "Metadata analysis of student reviews"
}

@article{ALHIJA200937,
title = {Student evaluation of instruction: What can be learned from students’ written comments?},
journal = {Studies in Educational Evaluation},
volume = {35},
number = {1},
pages = {37-44},
year = {2009},
issn = {0191-491X},
doi = {https://doi.org/10.1016/j.stueduc.2009.01.002},
url = {https://www.sciencedirect.com/science/article/pii/S0191491X09000066},
author = {Fadia Nasser-Abu Alhija and Barbara Fresko},
abstract = {Extensive research has been done on student ratings of instruction on closed-ended questionnaires, but little research has examined students’ written responses to open-ended questions. This study investigated the written comments of students in 198 classes, focusing on their frequency, content, direction, and consistency with quantitative ratings on closed-ended items. Results indicated that about 45% of the students wrote comments. Comments were more often positive than negative and tended to be general rather than specific. Written comments addressed dimensions similar to those identified in the closed-ended items, but they also related to unique aspects of the courses as well.}
}



@article{Zhao01032012,
author = {Jing Zhao and Dorinda J. Gallant},
title = {Student evaluation of instruction in higher education: exploring issues of validity and reliability},
journal = {Assessment \& Evaluation in Higher Education},
volume = {37},
number = {2},
pages = {227--235},
year = {2012},
publisher = {SRHE Website},
doi = {10.1080/02602938.2010.523819},
URL = {https://doi.org/10.1080/02602938.2010.523819},
eprint = { https://doi.org/10.1080/02602938.2010.523819},
    abstract = { Many personnel committees at colleges and universities in the USA use student evaluation of faculty instruction to make decisions regarding tenure, promotion, merit pay or faculty professional development. This study examines the construct validity and internal consistency reliability of the student evaluation of instruction (SEI) used at a large mid‐western university in the USA for both administrative and instructional purposes. The sample consisted of 73,500 completed SEIs for undergraduate students who self‐reported as freshman, sophomore, junior or senior. Confirmatory factor analysis via structural equation modelling was used to explore the construct validity of the SEI instrument. The internal consistency of students' ratings was reported to provide reliability evidence. The results of this study showed that the model fits the data for the sample. The significance of this study as well as areas for further research are discussed. }
}

@article{kennedy2018evaluation,
  title={An evaluation of the 2016 election polls in the United States},
  author={Kennedy, Courtney and Blumenthal, Mark and Clement, Scott and Clinton, Joshua D and Durand, Claire and Franklin, Charles and McGeeney, Kyley and Miringoff, Lee and Olson, Kristen and Rivers, Douglas and others},
  journal={Public Opinion Quarterly},
  volume={82},
  number={1},
  pages={1--33},
  year={2018},
  publisher={Oxford University Press US}
}


@article{Hamilton01072002,
author = {Diane M. Hamilton and Robert E. Pritchard and Carol N. Welsh and Gregory C. Potter and Michael S. Saccucci},
title = {The Effects of Using In-Class Focus Groups on Student Course Evaluations},
journal = {Journal of Education for Business},
volume = {77},
number = {6},
pages = {329--333},
year = {2002},
publisher = {Routledge},
doi = {10.1080/08832320209599684},


URL = { 
    
        https://doi.org/10.1080/08832320209599684
    
    

},
eprint = { 
    
        https://doi.org/10.1080/08832320209599684
    
    

}

}



@article{Smithson01012000,
author = {Janet Smithson},
title = {Using and analysing focus groups: Limitations and possibilities},
journal = {International Journal of Social Research Methodology},
volume = {3},
number = {2},
pages = {103--119},
year = {2000},
publisher = {Routledge},
doi = {10.1080/136455700405172},


URL = { 
    
        https://doi.org/10.1080/136455700405172
    
    

},
eprint = { 
    
        https://doi.org/10.1080/136455700405172
    
    

}

}

